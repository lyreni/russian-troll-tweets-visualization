{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting subject of sentence\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "sent = \"\"\n",
    "doc=nlp(sent)\n",
    "\n",
    "sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
    "print(sub_toks)\n",
    "nltk.download()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    # input: string\n",
    "    # output: string\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords \n",
    "    import string\n",
    "    lemma = WordNewLemmatizer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude or ch == \"!\")\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split()).split()\n",
    "    link_free = [word for word in normalized if 'http' not in word]\n",
    "    cleaned = \" \".join(word for word in link_free)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099*\"sugar\" + 0.069*\"sister\" + 0.069*\"father\"\n",
      "['sugar', 'sister', 'father'] [0.099, 0.069, 0.069]\n",
      "[('sugar', 0.099), ('sister', 0.069), ('father', 0.069)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n",
    "#doc_complete = [doc2]\n",
    "\n",
    "\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "#print(doc_clean)\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import re\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "#topics = ldamodel.print_topics(num_topics=3, num_words=3)\n",
    "topics = ldamodel.show_topics(num_topics=3, num_words=3)\n",
    "print(topics[0][1])\n",
    "percents = topics[0][1].split()\n",
    "percents = [i for i in percents if i != \"+\"]\n",
    "percents = [float(i[:5]) for i in percents]\n",
    "topics = re.findall('\"([^\"]*)\"', topics[0][1])\n",
    "to_return = zip(topics, percents)\n",
    "to_return = list(to_return)\n",
    "\n",
    "print(topics, percents)\n",
    "print(to_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train topic classifier\n",
    "\n",
    "def trainLDA(tweets):\n",
    "    # input: list of strings\n",
    "    # output: lda model trained on strings\n",
    "    import gensim\n",
    "    from gensim import corpora\n",
    "    # input: multiple strings\n",
    "    # output: trained ldamodel\n",
    "    tweets_clean = [clean(tweet).split() for tweet in tweets]\n",
    "    dictionary = corpora.Dictionary(tweets_clean)\n",
    "    tweet_term_matrix = [dictionary.doc2bow(tweet) for tweet in tweets_clean]\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    ldamodel = Lda(tweet_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "    return ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get topics of specific tweet\n",
    "\n",
    "def topics(tweets, ldamodel, num_words):\n",
    "    # input:\n",
    "    #     tweets: strings\n",
    "    #     ldamodel: Lda model from trainLDA\n",
    "    #     topics: integer number of topics to return per tweet\n",
    "    #     num_words: integer number of words per topic\n",
    "    # output:\n",
    "    #     list of lists, each list within the list contains a tuple (topic, percentage)\n",
    "    #     each list within the greater list is for each tweet\n",
    "    import re\n",
    "    \n",
    "    to_return = []\n",
    "    for tweet in tweets:\n",
    "        topics_w_percents = ldamodel.print_topics(num_topics=1, num_words=num_words)\n",
    "\n",
    "        percents = topics_w_percents[0][1].split()\n",
    "        #print(percents)\n",
    "\n",
    "        for_topics = [i for i in percents if i != \"+\"]\n",
    "        \n",
    "        percents = [float(i[:5]) for i in for_topics]\n",
    "        for_topics = \" \".join(word for word in for_topics)\n",
    "        topics = re.findall('\"([^\"]*)\"', for_topics)\n",
    "        topics_percents_tuples = zip(topics, percents)\n",
    "        topics_percents_tuples = list(topics_percents_tuples)\n",
    "        to_return.append(topics_percents_tuples)\n",
    "    \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting sentiment of sentence\n",
    "\n",
    "\n",
    "def sentiment(tweets):\n",
    "    # input: list of strings\n",
    "    # output: list of ints\n",
    "    import preprocessor as p\n",
    "    import nltk\n",
    "    import string as st\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    sentiments = []\n",
    "    for tweet in tweets:\n",
    "        tweet = clean(tweet)\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        score = sid.polarity_scores(tweet)\n",
    "        sentiments.append(score['compound'])\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6369, 0.3832, -0.6249, -0.0952]\n",
      "[('worst', 0.078), ('love', 0.078), ('little', 0.077)]\n",
      "[('thanksgiving', 0.118), ('unlucky!', 0.118), ('fail', 0.118)]\n",
      "[('thanksgiving', 0.118), ('unlucky!', 0.118), ('fail', 0.118)]\n",
      "[('worst', 0.078), ('love', 0.078), ('little', 0.077)]\n",
      "[[('worst', 0.078), ('love', 0.078), ('little', 0.077)], [('thanksgiving', 0.118), ('unlucky!', 0.118), ('fail', 0.118)], [('thanksgiving', 0.118), ('unlucky!', 0.118), ('fail', 0.118)], [('worst', 0.078), ('love', 0.078), ('little', 0.077)]]\n"
     ]
    }
   ],
   "source": [
    "tweet1 = \"I love you\"\n",
    "tweet2 = \"My day is always a little #better with some #wine\"\n",
    "tweet3 = \"you're the worst\"\n",
    "tweet4 = 'wooow Happy Thanksgiving  You really are unlucky! #Fail https://t.co/lUMFCOtksX'\n",
    "tweets = [tweet1,tweet2,tweet3,tweet4]\n",
    "print(sentiment(tweets))\n",
    "ldamodel = trainLDA(tweets)\n",
    "print(topics(tweets, ldamodel, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
